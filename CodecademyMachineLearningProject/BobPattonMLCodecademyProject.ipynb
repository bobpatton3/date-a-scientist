{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'profiles.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7bd8a3a81ef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"profiles.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'profiles.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.job.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.age, bins=54)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(16, 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.height, bins=90)\n",
    "plt.xlabel(\"Height\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(55, 85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.income.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drink_mapping = {\"not at all\": 0, \"rarely\": 1, \"socially\": 2, \"often\": 3, \"very often\": 4, \"desperately\": 5}\n",
    "df[\"drinks_code\"] = df.drinks.map(drink_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smokes_mapping = {\"no\": 0, \"trying to quit\": 1, \"sometimes\": 2, \"when drinking\": 3, \"yes\": 4}\n",
    "df[\"smokes_code\"] = df.smokes.map(smokes_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_mapping = {\"never\": 0, \"sometimes\": 1, \"often\": 2}\n",
    "df[\"drugs_code\"] = df.drugs.map(drugs_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_mapping = {\"m\": 0, \"f\": 1}\n",
    "df[\"sex_code\"] = df.sex.map(sex_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = { \"dropped out of space camp\": 0, \"working on space camp\": 1, \"space camp\": 2, \"graduated from space camp\": 3, \n",
    "                     \"dropped out of high school\": 4, \"working on high school\": 5, \"high school\": 6, \"graduated from high school\": 7, \n",
    "                     \"dropped out of two-year college\": 8, \"dropped out of college/university\": 9, \"working on two-year college\": 10, \n",
    "                     \"two-year college\": 11, \"graduated from two-year college\": 12, \"working on college/university\": 13, \n",
    "                     \"college/university\": 14, \"graduated from college/university\": 15, \"dropped out of masters program\": 16, \n",
    "                     \"dropped out of law school\": 17, \"dropped out of ph.d program\": 18, \"dropped out of med school\": 19, \n",
    "                     \"working on masters program\": 20, \"working on law school\": 21, \"masters program\": 22, \"law school\": 23, \n",
    "                     \"graduated from masters program\": 24, \"graduated from law school\": 25, \"working on ph.d program\": 26, \n",
    "                     \"working on med school\": 27, \"ph.d program\": 28, \"med school\": 29, \"graduated from ph.d program\": 30, \n",
    "                     \"graduated from med school\": 31,}\n",
    "df[\"education_code\"] = df.education.map(education_mapping)\n",
    "\n",
    "# trying to rationalize these into some sort of hierarchy is probably mis-guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_decade\"] = [math.floor(a/10) for a in df[\"age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_stage\"] = [0 if a < 36 else 1 for a in df[\"age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_cols = [\"essay0\",\"essay1\",\"essay2\",\"essay3\",\"essay4\",\"essay5\",\"essay6\",\"essay7\",\"essay8\",\"essay9\"]\n",
    "\n",
    "all_essays = df[essay_cols].replace(np.nan, '', regex=True)\n",
    "print(all_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_essays.apply(lambda x: ' '.join(x), axis=1)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_len = x.apply(lambda x: len(x))\n",
    "df['essay_len'] = essay_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means1 = [0 if re.search(\"[a-zA-Z]+\", s) == None else mean([len(w) for w in re.findall(\"[a-zA-Z]+\", s)]) for s in x]\n",
    "#print(means1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_len'] = means1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lists = [re.findall(\"[a-zA-Z]+\", s) for s in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_counts = [l.count(\"I\") + l.count(\"i\") + l.count(\"me\") + l.count(\"Me\") + l.count(\"ME\") for l in words_lists]\n",
    "#print(me_counts)\n",
    "df['I_me_counts'] = me_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data0 = df[['smokes_code', 'drinks_code', 'drugs_code', 'essay_len', 'word_len','I_me_counts','education_code','income','sign']]\n",
    "feature_data0 = feature_data0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = feature_data0[['smokes_code', 'drinks_code', 'drugs_code', 'essay_len', 'word_len','I_me_counts','education_code','income']]\n",
    "#print(feature_data)\n",
    "x = feature_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data1 = feature_data0[['smokes_code', 'drinks_code', 'drugs_code', 'essay_len', 'word_len','I_me_counts','education_code']]\n",
    "x1 = feature_data1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "#print(x_scaled)\n",
    "feature_data = pd.DataFrame(x_scaled, columns=feature_data.columns)\n",
    "#print(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_scaled = min_max_scaler.fit_transform(x1)\n",
    "feature_data1 = pd.DataFrame(x1_scaled, columns=feature_data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Scaled Education Code\")\n",
    "plt.ylabel(\"Scaled Essay Length\")\n",
    "plt.scatter(feature_data['education_code'], feature_data['essay_len'], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Scaled Education Code\")\n",
    "plt.ylabel(\"Scaled I/me Counts\")\n",
    "plt.scatter(feature_data['education_code'], feature_data['I_me_counts'], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Education Code\")\n",
    "plt.ylabel(\"I/me Counts\")\n",
    "plt.scatter(df['education_code'], df['I_me_counts'], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Drugs Code\")\n",
    "plt.ylabel(\"Drinks Code\")\n",
    "plt.scatter(df['drugs_code'], df['drinks_code'], alpha=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Education Code\")\n",
    "plt.ylabel(\"Word Length\")\n",
    "plt.ylim(0, 20)\n",
    "plt.scatter(df['education_code'], df['word_len'], alpha=0.1)\n",
    "# The distribution of word lengths _do_ stretch into the higher ranges for\n",
    "# those who graduated from college and graduated from a masters program (codes 15 and 24).\n",
    "# Hard to say much else about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data2 = df[['smokes_code', 'drinks_code', 'drugs_code', 'essay_len', 'word_len','I_me_counts','education_code','income']]\n",
    "feature_data2 = feature_data2.dropna()\n",
    "feature_data2 = feature_data2[feature_data2['income'] >= 0]\n",
    "feature_data2b = feature_data2[['smokes_code', 'drinks_code', 'drugs_code', 'essay_len', 'word_len','I_me_counts','education_code']]\n",
    "scaled_feature_data2b = min_max_scaler2.fit_transform(feature_data2b.values)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "model = regr.fit(scaled_feature_data2b, feature_data2['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#income_predict = regr.predict(scaled_feature_data2b)\n",
    "\n",
    "print(model.coef_)\n",
    "print(regr.score(scaled_feature_data2b, feature_data2['income']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=45)\n",
    "model = classifier.fit(feature_data, feature_data0['sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zodiac_predict = classifier.predict(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(feature_data0['sign'], zodiac_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(feature_data0['sign'], zodiac_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(feature_data0['sign'], zodiac_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(feature_data0['sign'], zodiac_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data10 = df[['essay_len', 'word_len','income']]\n",
    "feature_data10 = feature_data10.dropna()\n",
    "min_max_scaler2 = preprocessing.MinMaxScaler()\n",
    "\n",
    "positive_income_feature_data = feature_data10[feature_data10['income'] > 0]\n",
    "x2 = min_max_scaler2.fit_transform(positive_income_feature_data.values)\n",
    "\n",
    "positive_income_feature_data_scaled = pd.DataFrame(x2, columns=positive_income_feature_data.columns)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "model_income_from_essays = regr.fit(positive_income_feature_data_scaled[['essay_len','word_len']], positive_income_feature_data['income'])\n",
    "income_predict_from_essays = regr.predict(positive_income_feature_data_scaled[['essay_len','word_len']])\n",
    "\n",
    "print(model_income_from_essays.coef_)\n",
    "regr_score = regr.score(positive_income_feature_data_scaled[['essay_len','word_len']], positive_income_feature_data['income'])\n",
    "print(regr_score)\n",
    "print(income_predict_from_essays)\n",
    "print(positive_income_feature_data['income'])\n",
    "\n",
    "# Multiple Linear Regression prediction of income from essay length and word length sucks! \n",
    "# Not enough data since less than 12K rows reported income - which is only 20% of the dataset\n",
    "# If it is this bad on the training data then there is no reason to go back and do a test-train split (like I should have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data11 = df[['I_me_counts','age','age_decade']]\n",
    "feature_data11 = feature_data11.dropna()\n",
    "feature_data11 = feature_data11[feature_data11['I_me_counts'] > 0]\n",
    "\n",
    "features = feature_data11[['I_me_counts']]\n",
    "labels = feature_data11[['age']]\n",
    "\n",
    "features_scaled = pd.DataFrame(min_max_scaler2.fit_transform(features.values), columns=features.columns)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "model_age_from_essays = regr.fit(features_scaled, labels)\n",
    "age_predict_from_essays = regr.predict(features_scaled)\n",
    "\n",
    "print(model_age_from_essays.coef_)\n",
    "regr_score = regr.score(features_scaled, labels)\n",
    "print(regr_score)\n",
    "print(age_predict_from_essays)\n",
    "#print(labels)\n",
    "\n",
    "# Linear Regression prediction of age from essay usage of \"I\" and \"me\" sucks!\n",
    "# simply no significant correlation as can be seen from the following graph \n",
    "# (since this is a trivial model with just a single feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"I/me Counts\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.scatter(feature_data11['I_me_counts'], feature_data11['age'], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(0, 10000)\n",
    "plt.hist(df.essay_len, bins=1000)\n",
    "plt.xlabel(\"essay_len\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_data11a = df[['essay_len','word_len','I_me_counts','age','age_decade']]\n",
    "feature_data11a = feature_data11a.dropna()\n",
    "feature_data11a = feature_data11a[feature_data11a['essay_len'] > 50]\n",
    "feature_data11a = feature_data11a[feature_data11a['word_len'] > 0]\n",
    "feature_data11a = feature_data11a[feature_data11a['I_me_counts'] > 0]\n",
    "\n",
    "features = feature_data11a[['essay_len','word_len','I_me_counts']]\n",
    "labels = feature_data11a[['age']]\n",
    "\n",
    "features_scaled = pd.DataFrame(min_max_scaler2.fit_transform(features.values), columns=features.columns)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "model_age_from_essays = regr.fit(features_scaled, labels)\n",
    "age_predict_from_essays = regr.predict(features_scaled)\n",
    "\n",
    "print(model_age_from_essays.coef_)\n",
    "regr_score = regr.score(features_scaled, labels)\n",
    "print(regr_score)\n",
    "#print(age_predict_from_essays)\n",
    "#print(labels)\n",
    "\n",
    "rmse = np.sqrt(((age_predict_from_essays - labels) ** 2).mean())\n",
    "print(\"------------------\")\n",
    "print(rmse)\n",
    "# Multiple Linear Regression prediction of age from all 3 essay features still sucks and\n",
    "# yet 5 times better than simply using I_me_counts!\n",
    "# It is mildly interesting that the coefficient for I_me_counts is negative suggesting \n",
    "# that greater usage of \"I\" and \"me\" in this model is indicative of younger ages. This \n",
    "# is also supported by the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(age_predict_from_essays.max())\n",
    "s = 1000\n",
    "for i in range(s, s+ 100):\n",
    "    print('{0} | {1}'.format(labels.iloc[i].age, age_predict_from_essays[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data11b = df[['essay_len','word_len','I_me_counts','education_code','age']]\n",
    "feature_data11b = feature_data11b.dropna()\n",
    "feature_data11b = feature_data11b[feature_data11b['essay_len'] > 50]\n",
    "feature_data11b = feature_data11b[feature_data11b['word_len'] > 0]\n",
    "feature_data11b = feature_data11b[feature_data11b['I_me_counts'] > 0]\n",
    "\n",
    "features = feature_data11b[['essay_len','word_len','I_me_counts','education_code']]\n",
    "labels = feature_data11b[['age']]\n",
    "\n",
    "features_scaled = pd.DataFrame(min_max_scaler2.fit_transform(features.values), columns=features.columns)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "model_age_from_essays = regr.fit(features_scaled, labels)\n",
    "age_predict_from_essays = regr.predict(features_scaled)\n",
    "\n",
    "print(model_age_from_essays.coef_)\n",
    "regr_score = regr.score(features_scaled, labels)\n",
    "print(regr_score)\n",
    "print(age_predict_from_essays)\n",
    "print(labels)\n",
    "\n",
    "# Throw in education and it gets better again. Still lousy though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data11c = df[['essay_len','word_len','I_me_counts','education_code','age']]\n",
    "feature_data11c = feature_data11c.dropna()\n",
    "feature_data11c = feature_data11c[feature_data11c['essay_len'] > 50]\n",
    "feature_data11c = feature_data11c[feature_data11c['word_len'] > 0]\n",
    "feature_data11c = feature_data11c[feature_data11c['I_me_counts'] > 0]\n",
    "\n",
    "x = feature_data11c[['essay_len','word_len','I_me_counts','education_code']]\n",
    "\n",
    "x_scaled = pd.DataFrame(min_max_scaler2.fit_transform(x.values), columns=x.columns)\n",
    "\n",
    "y = feature_data11c[['age']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, train_size=0.8, test_size=0.2, random_state=6)\n",
    "\n",
    "\n",
    "regr = KNeighborsRegressor(n_neighbors = 200, weights = \"distance\")\n",
    "model_age_from_essays = regr.fit(x_train, y_train['age'])\n",
    "age_predict_from_essays = regr.predict(x_test)\n",
    "\n",
    "regr_score = regr.score(x_test, y_test['age'])\n",
    "print(regr_score)\n",
    "#print(age_predict_from_essays)\n",
    "#print(y_test['age'])\n",
    "\n",
    "rmse = np.sqrt(((age_predict_from_essays - y_test['age']) ** 2).mean())\n",
    "print(\"------------------\")\n",
    "print(rmse)\n",
    "\n",
    "# Almost 3 times better than simple Linear Regression! But STILL lousy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data11d = df[['sex_code','education_code','income']]\n",
    "feature_data11d = feature_data11d.dropna()\n",
    "feature_data11d = feature_data11d[feature_data11d['income'] > 0]\n",
    "\n",
    "features = feature_data11d[['sex_code','education_code']]\n",
    "labels = feature_data11d[['income']]\n",
    "\n",
    "features_scaled = pd.DataFrame(min_max_scaler2.fit_transform(features.values), columns=features.columns)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "model_age_from_essays = regr.fit(features_scaled, labels)\n",
    "predict_from_essays = regr.predict(features_scaled)\n",
    "\n",
    "print(model_age_from_essays.coef_)\n",
    "regr_score = regr.score(features_scaled, labels)\n",
    "print(regr_score)\n",
    "print(age_predict_from_essays)\n",
    "#print(labels)\n",
    "\n",
    "# income from education and sex - nothing to report. Interesting that both \n",
    "# coefficients are negative but given its lousy predictability I can't say that it means much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Sex Code\")\n",
    "plt.ylabel(\"Income\")\n",
    "plt.scatter(feature_data11d['sex_code'], feature_data11d['income'], alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Education Code\")\n",
    "plt.ylabel(\"Income\")\n",
    "plt.ylim(0, 400000)\n",
    "plt.scatter(feature_data11d['education_code'], feature_data11d['income'], alpha=0.01)\n",
    "# Higher salaries _do_ appear to be correlated with codes 15 and 24 which are \"graduated \n",
    "# from college/university\" and \"graduated from masters program\". There just isn't enough \n",
    "# data points for code 31 \"graduated from medical school\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data11e = df[['essay_len','word_len','I_me_counts','income','sex']]\n",
    "feature_data11e = feature_data11e.dropna()\n",
    "feature_data11e = feature_data11e[feature_data11e['essay_len'] > 50]\n",
    "feature_data11e = feature_data11e[feature_data11e['word_len'] > 0]\n",
    "feature_data11e = feature_data11e[feature_data11e['I_me_counts'] > 0]\n",
    "\n",
    "x = feature_data11e[['essay_len','word_len','I_me_counts','income']]\n",
    "\n",
    "x_scaled = pd.DataFrame(min_max_scaler2.fit_transform(x.values), columns=x.columns)\n",
    "\n",
    "y = feature_data11e[['sex']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, train_size=0.8, test_size=0.2, random_state=6)\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=50)\n",
    "model = classifier.fit(x_train, y_train['sex'])\n",
    "model_predict = model.predict(x_test)\n",
    "k_data.append(model_score)\n",
    "\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_data = []\n",
    "k_data.append(0)\n",
    "for i in range(1,99):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=i)\n",
    "    model = classifier.fit(x_train, y_train['sex'])\n",
    "    model_score = model.score(x_test, y_test['sex'])\n",
    "    k_data.append(model_score)\n",
    "\n",
    "plt.plot(range(0,len(k_data)), k_data)\n",
    "\n",
    "# prediction is only slightly better than half! In other words - useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data11e = df[['essay_len','word_len','I_me_counts','income','age_decade']]\n",
    "feature_data11e = feature_data11e.dropna()\n",
    "feature_data11e = feature_data11e[feature_data11e['essay_len'] > 50]\n",
    "feature_data11e = feature_data11e[feature_data11e['word_len'] > 0]\n",
    "feature_data11e = feature_data11e[feature_data11e['I_me_counts'] > 0]\n",
    "\n",
    "x = feature_data11e[['essay_len','word_len','I_me_counts','income']]\n",
    "\n",
    "x_scaled = pd.DataFrame(min_max_scaler2.fit_transform(x.values), columns=x.columns)\n",
    "\n",
    "y = feature_data11e[['age_decade']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, train_size=0.8, test_size=0.2, random_state=6)\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=50)\n",
    "model = classifier.fit(x_train, y_train['age_decade'])\n",
    "model_predict = model.predict(x_test)\n",
    "k_data.append(model_score)\n",
    "\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_data = []\n",
    "k_data.append(0)\n",
    "for i in range(1,99):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=i)\n",
    "    model = classifier.fit(x_train, y_train['age_decade'])\n",
    "    model_score = model.score(x_test, y_test['age_decade'])\n",
    "    k_data.append(model_score)\n",
    "\n",
    "plt.plot(range(0,len(k_data)), k_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.age_decade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_predict.min())\n",
    "print(model_predict.max())\n",
    "model_predict2 = model.predict(x_train)\n",
    "print(model_predict2.min())\n",
    "print(model_predict2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_cols = [\"essay0\",\"essay1\",\"essay2\",\"essay3\",\"essay4\",\"essay5\",\"essay6\",\"essay7\",\"essay8\",\"essay9\"]\n",
    "all_essays = df[essay_cols].replace(np.nan, '', regex=True)\n",
    "x = all_essays.apply(lambda x: ' '.join(x), axis=1)\n",
    "df['essays_joined'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means1 = [[w for w in re.findall(\"[a-zA-Z]+\", s)] for s in x]\n",
    "\n",
    "words_lists = [re.findall(\"[a-zA-Z][a-zA-Z][a-zA-Z][a-zA-Z][a-zA-Z]+\", l) for l in x]\n",
    "unique_words = list(set(x for l in words_lists for x in l))\n",
    "\n",
    "# now use this in a NB classifier to see if we can predict sex or education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_words))\n",
    "essays_long_words = [\" \".join(l) for l in words_lists]\n",
    "df['essays_long_words'] = essays_long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data12 = df[['essay_len','essays_long_words','sex']]\n",
    "feature_data12 = feature_data12.dropna()\n",
    "feature_data12 = feature_data12[feature_data12['essay_len'] > 50]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data12[['essay_len','essays_long_words']], feature_data12[['sex']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "#print(training_counts)\n",
    "#print(y_train)\n",
    "classifier.fit(training_counts, y_train['sex'])\n",
    "\n",
    "model_predict = classifier.predict(test_counts)\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data13 = df[['essay_len','essays_long_words','education']]\n",
    "feature_data13 = feature_data13.dropna()\n",
    "feature_data13 = feature_data13[feature_data13['essay_len'] > 50]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data13[['essay_len','essays_long_words']], feature_data13[['education']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['education'])\n",
    "score = classifier.score(test_counts, y_test['education'])\n",
    "print(score)\n",
    "model_predict = classifier.predict(test_counts)\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data13a = df[['essay_len','essays_long_words','job']]\n",
    "feature_data13a = feature_data13a.dropna()\n",
    "feature_data13a = feature_data13a[feature_data13a['essay_len'] > 50]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data13a[['essay_len','essays_long_words']], feature_data13a[['job']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['job'])\n",
    "score = classifier.score(test_counts, y_test['job'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data13b = df[['essay_len','essays_long_words','age_decade']]\n",
    "feature_data13b = feature_data13b.dropna()\n",
    "feature_data13b = feature_data13b[feature_data13b['essay_len'] > 50]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data13b[['essay_len','essays_long_words']], feature_data13b[['age_decade']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['age_decade'])\n",
    "score = classifier.score(test_counts, y_test['age_decade'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data14 = df[['essay_len','essays_long_words','age_stage']]\n",
    "feature_data14 = feature_data14.dropna()\n",
    "feature_data14 = feature_data14[feature_data14['essay_len'] > 50]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data14[['essay_len','essays_long_words']], feature_data14[['age_stage']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['age_stage'])\n",
    "score = classifier.score(test_counts, y_test['age_stage'])\n",
    "print(score)\n",
    "# Not bad! Word choice has an 82% prediction rate for \"35 and under\" vs \"over 35\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = classifier.predict(test_counts)\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall and precision are both better for the classification \"younger\".\n",
    "This is easy to have happen if the NUMBER of mis-identified items are \n",
    "about the same for both classifications but the total number of \"younger\"\n",
    "items is greater than the total number of \"older\" items.\n",
    "The confusion matrix could be like:\n",
    "\n",
    "|&nbsp;| young    |   old|\n",
    "|----------------|----------|------|\n",
    "|predicted young |     17   |     3|\n",
    "|predicted old   |      3   |     6|\n",
    "\n",
    "... and result in values roughly approximately to those in the previous output:<br>\n",
    "\n",
    "|&nbsp;| young    |   old|\n",
    "|----------------|----------|------|\n",
    "|recall |     0.8811651   |     0.67666667|\n",
    "|precision   |      0.87093262   |     0.69694132|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data15 = df[['essay_len','essays_long_words','religion']]\n",
    "feature_data15 = feature_data15.dropna()\n",
    "feature_data15 = feature_data15[feature_data15['essay_len'] > 50]\n",
    "\n",
    "feature_data15[\"religion_serious\"] = ['not too' if (re.search('but not too serious', a) != None) \n",
    "     else 'laughing' if (re.search('and laughing', a) != None) \n",
    "     else 'very' if (re.search('and very serious', a) != None) \n",
    "     else 'somewhat' if (re.search('and somewhat serious', a) != None) \n",
    "     else 'no_qualifier' for a in feature_data15[\"religion\"]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data15[['essay_len','essays_long_words']], feature_data15[['religion_serious']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['religion_serious'])\n",
    "score = classifier.score(test_counts, y_test['religion_serious'])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data15[\"religion_base\"] = [re.search('(\\w+)', a).group(1) for a in feature_data15[\"religion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data15.religion_base.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_data15[['essay_len','essays_long_words']], feature_data15[['religion_base']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(unique_words)\n",
    "training_counts = counter.transform(x_train['essays_long_words'])\n",
    "test_counts = counter.transform(x_test['essays_long_words'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['religion_base'])\n",
    "score = classifier.score(test_counts, y_test['religion_base'])\n",
    "print(score)\n",
    "# 34% Not very good but I didn't expect much out of this one honestly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we ran a Naive Bayes model on the text for the \"religion\" column focusing only on the words:\n",
    "religion_words = ['agnosticism','atheism','other','too','very','somewhat','laughing']\n",
    "# In other words split it into non-religious (agnosticism, atheism, other - this last because I \n",
    "# think people use that when they aren't religious and just don't care) and \n",
    "# religious but delineated by level of seriousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data16 = df[['religion','drugs']]\n",
    "feature_data16 = feature_data16.dropna()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data16[['religion']], feature_data16[['drugs']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=42)\n",
    "counter2 = CountVectorizer()\n",
    "counter2.fit(religion_words)\n",
    "training_counts = counter2.transform(x_train['religion'])\n",
    "test_counts = counter2.transform(x_test['religion'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['drugs'])\n",
    "# score = classifier.score(test_counts, y_test['drugs'])\n",
    "# print(score)\n",
    "# About 81%! Not bad!\n",
    "\n",
    "model_predict = classifier.predict(test_counts)\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ i for i in model_predict if i != 'never']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data16.drugs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25167 / (25167 + 5860 + 304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above predictor ended up just always predicting 'never' for drug usage! Its accuracy appeared to be reasonably good just because 80.3% of respondents entered 'never'! This is just the kind of situation warned about as to why accuracy alone can't be trusted to give the full story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data17 = df[['religion','drinks']]\n",
    "feature_data17 = feature_data17.dropna()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data17[['religion']], feature_data17[['drinks']], \n",
    "                                                    train_size=0.8, test_size=0.2, random_state=6)\n",
    "counter2 = CountVectorizer()\n",
    "counter2.fit(religion_words)\n",
    "training_counts = counter2.transform(x_train['religion'])\n",
    "test_counts = counter2.transform(x_test['religion'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, y_train['drinks'])\n",
    "#score = classifier.score(test_counts, y_test['drinks'])\n",
    "#print(score)\n",
    "# About 71%! Also not too shabby.\n",
    "model_predict = classifier.predict(test_counts)\n",
    "accuracy = accuracy_score(y_test, model_predict)\n",
    "print(accuracy)\n",
    "recall = recall_score(y_test, model_predict, average=None)\n",
    "print(recall)\n",
    "precision = precision_score(y_test, model_predict, average=None)\n",
    "print(precision)\n",
    "f1 = f1_score(y_test, model_predict, average=None)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data17.drinks.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ i for i in model_predict if i != 'socially']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same problem as above. The model looks good if we only look at the accuracy but it is always just reporting the most common value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
